# -*- coding: utf-8 -*-
"""TaskSubmission_internid.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-Cvws5Im1HxGBV0G0Gkpn5Jp3FHaIua1

# **One vs Rest Classification**
"""

# Mount Google Drive to access the dataset
from google.colab import drive
drive.mount('/content/drive')

import os
import cv2
import numpy as np
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import layers, models
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Function to load images and labels from the dataset
def load_images_and_labels(dataset_path):
    images = []
    labels = []
    label_mapping = {}
    label_count = 0

    for folder in os.listdir(dataset_path):
        label_mapping[label_count] = folder
        for file in os.listdir(os.path.join(dataset_path, folder)):
            img_path = os.path.join(dataset_path, folder, file)
            img = cv2.imread(img_path)
            img = cv2.resize(img, (224, 224))  # Adjust size as needed
            images.append(img)
            labels.append(label_count)
        label_count += 1

    return np.array(images), np.array(labels), label_mapping

# Specify your dataset path
dataset_path = '/content/drive/MyDrive/animals'

# Load images and labels
images, labels, label_mapping = load_images_and_labels(dataset_path)

# Convert labels to one-hot encoding
le = LabelEncoder()
labels_encoded = le.fit_transform(labels)
labels_one_hot = to_categorical(labels_encoded)

# Initialize variables to accumulate results
average_conf_mat = np.zeros((len(label_mapping), len(label_mapping)))
average_accuracy = 0.0

# Create a KFold object for 3-fold cross-validation
kf = KFold(n_splits=3, shuffle=True, random_state=42)

from sklearn.model_selection import train_test_split
# Iterate over the folds
for fold_idx, (train_index, val_test_index) in enumerate(kf.split(images)):
    print(f'\nFold {fold_idx + 1}:')

    X_train, X_val_test = images[train_index], images[val_test_index]
    y_train, y_val_test = labels_one_hot[train_index], labels_one_hot[val_test_index]

    # Split the remaining data into validation and test sets
    val_index, test_index = train_test_split(np.arange(len(X_val_test)), test_size=0.5, random_state=42)
    X_val, X_test = X_val_test[val_index], X_val_test[test_index]
    y_val, y_test = y_val_test[val_index], y_val_test[test_index]


    # Build a custom CNN model
    model = models.Sequential()
    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(32, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(32, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(128, (3, 3), activation='relu'))
    model.add(layers.Conv2D(128, (3, 3), activation='relu'))
    model.add(layers.Conv2D(128, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Flatten())
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.Dense(512, activation='relu'))
    model.add(layers.Dense(90, activation='softmax'))

    # Compile the model
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    # Train the model
    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=16)
    #to be run for atleast 200 epochs for good results)

    # Evaluate the model on the test set
    test_loss, test_acc = model.evaluate(X_test, y_test)
    print(f'Test Accuracy: {test_acc}')

    # Generate classification matrices for visualization (confusion matrix)
    # Predictions on test set
    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true_classes = np.argmax(y_test, axis=1)

    # Confusion matrix for the current fold
    conf_mat = confusion_matrix(y_true_classes, y_pred_classes)

    # Accumulate results
    average_conf_mat += conf_mat
    average_accuracy += test_acc

# Calculate average accuracy across folds
average_accuracy /= 3

# Plot the average confusion matrix
average_conf_mat /= 3  # Calculate average
plt.figure(figsize=(8, 8))
sns.heatmap(average_conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=label_mapping.values(), yticklabels=label_mapping.values())
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title(f'Average Confusion Matrix (Average Accuracy: {average_accuracy:.4f})')
plt.show()

#Convolutional Layer Visualization
layer_outputs = [layer.output for layer in model.layers[:3]]  # Extract the outputs of the first 3 layers
activation_model = models.Model(inputs=model.input, outputs=layer_outputs)  # Create a model that will return these outputs
activations = activation_model.predict(X_val[0].reshape(1, 224, 224, 3))  # Get the activations of the first validation image

# Plot the outputs of each convolutional layer
for i, activation in enumerate(activations):
    plt.figure(figsize=(8, 8))
    plt.matshow(activation[0, :, :, 1], cmap='viridis')  # Display the feature map for the first channel
    plt.title(f'Activation of Layer {i + 1}')
    plt.show()

"""# **5-Class Classification**"""

import os
import cv2
import numpy as np
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Function to load images and labels from the dataset
def load_images_and_labels(dataset_path):
    images = []
    labels = []
    label_mapping = {}
    label_count = 0

    for folder in os.listdir(dataset_path):
        label_mapping[label_count] = folder
        for file in os.listdir(os.path.join(dataset_path, folder)):
            img_path = os.path.join(dataset_path, folder, file)
            img = cv2.imread(img_path)
            img = cv2.resize(img, (224, 224))  # Adjust size as needed
            images.append(img)
            labels.append(label_count)
        label_count += 1

    return np.array(images), np.array(labels), label_mapping

# Specify your dataset path
dataset_path = '/content/drive/MyDrive/animals'
# Load images and labels
images, labels, label_mapping = load_images_and_labels(dataset_path)

# Keep only 5 classes
selected_classes = list(label_mapping.keys())[:5]
selected_images = []
selected_labels = []
for idx, label in enumerate(labels):
    if label in selected_classes:
        selected_images.append(images[idx])
        selected_labels.append(label)

selected_labels_encoded = to_categorical(LabelEncoder().fit_transform(selected_labels))

# Create a KFold object for 3-fold cross-validation
kf = KFold(n_splits=3, shuffle=True, random_state=42)

# Lists to store results from each fold
all_confusion_matrices = []
all_accuracies = []

# Iterate over the folds
for fold_idx, (train_index, val_index) in enumerate(kf.split(selected_images)):
    print(f'\nFold {fold_idx + 1}:')

    X_train, X_val = selected_images[train_index], selected_images[val_index]
    y_train, y_val = selected_labels_encoded[train_index], selected_labels_encoded[val_index]

    # Build a custom CNN model
    model = models.Sequential()
    model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(224, 224, 3)))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(128, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(256, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Flatten())
    model.add(layers.Dense(512, activation='relu'))
    model.add(layers.Dropout(0.5))
    model.add(layers.Dense(len(selected_classes), activation='softmax'))

    # Use Adam optimizer with a lower learning rate
    model.compile(optimizer=optimizers.Adam(learning_rate=0.0001),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    # Train the model
    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)

    # Evaluate the model
    test_loss, test_acc = model.evaluate(X_val, y_val)
    print(f'Test Accuracy: {test_acc}')

    # Generate classification matrices for visualization (confusion matrix)
    # Predictions on validation set
    y_pred = model.predict(X_val)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true_classes = np.argmax(y_val, axis=1)

    # Confusion matrix
    conf_mat = confusion_matrix(y_true_classes, y_pred_classes)

    # Save results from this fold
    all_confusion_matrices.append(conf_mat)
    all_accuracies.append(test_acc)

# Calculate average confusion matrix and accuracy
average_confusion_matrix = np.mean(all_confusion_matrices, axis=0)
average_accuracy = np.mean(all_accuracies)

# Display average confusion matrix
plt.figure(figsize=(8, 8))
sns.heatmap(average_confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_mapping.values(), yticklabels=label_mapping.values())
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title(f'Average Confusion Matrix (3 Folds)')
plt.show()

print(f'Average Accuracy: {average_accuracy}')

# Convolutional Layer Visualization (assuming 3 convolutional layers)
layer_outputs = [layer.output for layer in model.layers[:3]]
activation_model = models.Model(inputs=model.input, outputs=layer_outputs)
activations = activation_model.predict(X_val[0].reshape(1, 224, 224, 3))

# Plot the outputs of each convolutional layer
for i, activation in enumerate(activations):
    plt.figure(figsize=(8, 8))
    plt.matshow(activation[0, :, :, 1], cmap='viridis')
    plt.title(f'Activation of Layer {i + 1}')
    plt.show()